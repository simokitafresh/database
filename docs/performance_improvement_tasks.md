# APIé«˜é€ŸåŒ–å®Ÿè£…ã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆ

## æ¦‚è¦
- **ç›®æ¨™**: 10éŠ˜æŸ„åŒæ™‚ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®å¿œç­”é€Ÿåº¦ã‚’10-15ç§’ã‹ã‚‰2-3ç§’ã«çŸ­ç¸®
- **ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒå¯¾è±¡**: TQQQ, TECL, GLD, XLU, ^VIX, QQQ, SPY, TMV, TMF, LQD
- **å®Ÿè£…æ–¹é‡**: ä¸¦è¡Œå‡¦ç†ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã€ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒã®3æ®µéšå®Ÿè£…

## ç¾åœ¨ã®ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹æ§‹é€ 
- **FastAPI ã‚¢ãƒ—ãƒª**: `app/main.py` (lifespanç®¡ç†ã‚ã‚Š)
- **è¨­å®šç®¡ç†**: `app/core/config.py` (Settings ã‚¯ãƒ©ã‚¹ã§ç’°å¢ƒå¤‰æ•°ç®¡ç†)
- **ä¾¡æ ¼å–å¾—API**: `app/api/v1/prices.py` (get_prices ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ)
- **ãƒ‡ãƒ¼ã‚¿å–å¾—**: `app/services/fetcher.py` (fetch_prices é–¢æ•° - yfinanceåˆ©ç”¨)
- **DBå‡¦ç†**: `app/db/queries.py` (ensure_coverage, get_prices_resolved)
- **æ­£è¦åŒ–**: `app/services/normalize.py` (normalize_symbol é–¢æ•°)
- **ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†**: `app/api/deps.py` (get_session, get_db)
- **DBã‚¨ãƒ³ã‚¸ãƒ³**: `app/db/engine.py` (create_engine_and_sessionmaker)

## ã‚¿ã‚¹ã‚¯ä¸€è¦§

### Phase 1: ç’°å¢ƒå¤‰æ•°ã¨è¨­å®šã®æ›´æ–°ï¼ˆä¾å­˜ãªã—ï¼‰

#### TASK-001: ç’°å¢ƒå¤‰æ•°ã®æ›´æ–° â¬œ
**è²¬ä»»**: æ¥ç¶šãƒ—ãƒ¼ãƒ«ã¨APIåˆ¶é™ã®æ‹¡å¼µ
**å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«**: `app/core/config.py`
**ç¾åœ¨ã®å€¤**: 
- DB_POOL_SIZE = 2
- DB_MAX_OVERFLOW = 3  
- YF_REQ_CONCURRENCY = 2
- API_MAX_SYMBOLS = 5

**å®Ÿè£…å†…å®¹**:
```python
# app/core/config.py ã®Settingsã‚¯ãƒ©ã‚¹ã«ä»¥ä¸‹ã‚’è¿½åŠ /æ›´æ–°
class Settings(BaseSettings):
    # æ—¢å­˜ã®è¨­å®šï¼ˆå€¤ã‚’æ›´æ–°ï¼‰
    DB_POOL_SIZE: int = 10  # 2ã‹ã‚‰å¤‰æ›´
    DB_MAX_OVERFLOW: int = 10  # 3ã‹ã‚‰å¤‰æ›´
    YF_REQ_CONCURRENCY: int = 8  # 2ã‹ã‚‰å¤‰æ›´
    API_MAX_SYMBOLS: int = 10  # 5ã‹ã‚‰å¤‰æ›´
    
    # æ–°è¦è¿½åŠ ã®è¨­å®š
    CACHE_TTL_SECONDS: int = 60
    ENABLE_CACHE: bool = True
    PREFETCH_SYMBOLS: str = "TQQQ,TECL,GLD,XLU,^VIX,QQQ,SPY,TMV,TMF,LQD"
    PREFETCH_INTERVAL_MINUTES: int = 5
    
    # æ—¢å­˜ã®model_configã¯ãã®ã¾ã¾ç¶­æŒ
    model_config = SettingsConfigDict(env_file=".env", env_file_encoding="utf-8", extra="ignore")
```

**ãƒ†ã‚¹ãƒˆæ–¹æ³•**:
```python
from app.core.config import settings
assert settings.DB_POOL_SIZE == 10
assert settings.ENABLE_CACHE == True
assert "TQQQ" in settings.PREFETCH_SYMBOLS
```

**å®Œäº†æ¡ä»¶**: 
- ã‚¢ãƒ—ãƒªèµ·å‹•æ™‚ã«ã‚¨ãƒ©ãƒ¼ãªã—
- `settings.PREFETCH_SYMBOLS`ã‹ã‚‰éŠ˜æŸ„ãƒªã‚¹ãƒˆãŒå–å¾—å¯èƒ½

#### TASK-002: render.yaml ã®æ›´æ–° â¬œ
**è²¬ä»»**: æœ¬ç•ªç’°å¢ƒå¤‰æ•°ã®è¨­å®š
**å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«**: `render.yaml`ï¼ˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆï¼‰
**ç¾åœ¨ã®è¨­å®š**: DB_POOL_SIZE=2, YF_REQ_CONCURRENCY=2 ãªã©

**å®Ÿè£…å†…å®¹**:
```yaml
# render.yaml ã®envVarsã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«ä»¥ä¸‹ã‚’è¿½åŠ /æ›´æ–°
services:
  - type: web
    name: stockdata-api
    envVars:
      # æ—¢å­˜ã®è¨­å®šã‚’æ›´æ–°
      - key: DB_POOL_SIZE
        value: "10"  # 2ã‹ã‚‰å¤‰æ›´
      - key: DB_MAX_OVERFLOW
        value: "10"  # 3ã‹ã‚‰å¤‰æ›´
      - key: YF_REQ_CONCURRENCY
        value: "8"   # 2ã‹ã‚‰å¤‰æ›´
      - key: API_MAX_SYMBOLS
        value: "10"  # ç¾åœ¨ã®å€¤ã‚’ç¢ºèªã—ã¦æ›´æ–°
      
      # æ–°è¦è¿½åŠ 
      - key: CACHE_TTL_SECONDS
        value: "60"
      - key: ENABLE_CACHE
        value: "true"
      - key: PREFETCH_SYMBOLS
        value: "TQQQ,TECL,GLD,XLU,^VIX,QQQ,SPY,TMV,TMF,LQD"
      - key: PREFETCH_INTERVAL_MINUTES
        value: "5"
```

**ãƒ†ã‚¹ãƒˆæ–¹æ³•**: 
```bash
# YAMLã®æ§‹æ–‡ãƒã‚§ãƒƒã‚¯
python -c "import yaml; yaml.safe_load(open('render.yaml'))"
```

**å®Œäº†æ¡ä»¶**: YAMLãƒ•ã‚¡ã‚¤ãƒ«ãŒæœ‰åŠ¹ã§Renderã«ãƒ‡ãƒ—ãƒ­ã‚¤å¯èƒ½

### Phase 2: ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŸºç›¤å®Ÿè£…ï¼ˆä¾å­˜: Phase 1ï¼‰

#### TASK-003: ã‚¤ãƒ³ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒ©ã‚¹ã®å®Ÿè£… â¬œ
**è²¬ä»»**: TTLä»˜ãã‚¤ãƒ³ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥
**å…¥åŠ›**: ãªã—
**å‡ºåŠ›**: app/services/cache.py
**å®Ÿè£…**:
```python
# app/services/cache.py
from typing import Any, Optional, Dict, Tuple
from datetime import datetime, timedelta
import asyncio
from threading import RLock

class InMemoryCache:
    def __init__(self, ttl_seconds: int = 60, max_size: int = 1000):
        self._cache: Dict[str, Tuple[Any, datetime]] = {}
        self._ttl = ttl_seconds
        self._max_size = max_size
        self._lock = RLock()
    
    async def get(self, key: str) -> Optional[Any]:
        with self._lock:
            if key in self._cache:
                value, timestamp = self._cache[key]
                if datetime.utcnow() - timestamp < timedelta(seconds=self._ttl):
                    return value
                else:
                    del self._cache[key]
        return None
    
    async def set(self, key: str, value: Any) -> None:
        with self._lock:
            if len(self._cache) >= self._max_size:
                oldest_key = min(self._cache.keys(), 
                               key=lambda k: self._cache[k][1])
                del self._cache[oldest_key]
            self._cache[key] = (value, datetime.utcnow())
    
    async def clear(self) -> None:
        with self._lock:
            self._cache.clear()
    
    def get_sync(self, key: str) -> Optional[Any]:
        """åŒæœŸç‰ˆï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰"""
        return asyncio.run(self.get(key))
```
**ãƒ†ã‚¹ãƒˆ**: set/get/TTLæœŸé™åˆ‡ã‚Œã®ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆ
**å®Œäº†æ¡ä»¶**: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®åŸºæœ¬æ“ä½œãŒå‹•ä½œ

#### TASK-004: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆ â¬œ
**è²¬ä»»**: ã‚¢ãƒ—ãƒªå…¨ä½“ã§å…±æœ‰ã•ã‚Œã‚‹ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
**å…¥åŠ›**: TASK-003
**å‡ºåŠ›**: app/services/cache.py ã®æ›´æ–°
**å®Ÿè£…**:
```python
# app/services/cache.py ã«è¿½åŠ 
_cache_instance: Optional[InMemoryCache] = None

def get_cache() -> InMemoryCache:
    global _cache_instance
    if _cache_instance is None:
        from app.core.config import settings
        _cache_instance = InMemoryCache(
            ttl_seconds=settings.CACHE_TTL_SECONDS,
            max_size=1000
        )
    return _cache_instance
```
**ãƒ†ã‚¹ãƒˆ**: get_cache() ãŒåŒä¸€ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’è¿”ã™ã“ã¨ã‚’ç¢ºèª
**å®Œäº†æ¡ä»¶**: ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒæ©Ÿèƒ½

### Phase 3: æ­£è¦åŒ–å‡¦ç†ã®å¼·åŒ–ï¼ˆä¾å­˜: ãªã—ï¼‰

#### TASK-005: ç‰¹æ®Šã‚·ãƒ³ãƒœãƒ«ï¼ˆ^VIXï¼‰å¯¾å¿œ â¬œ
**è²¬ä»»**: ^ã§å§‹ã¾ã‚‹æŒ‡æ•°ã‚·ãƒ³ãƒœãƒ«ã®æ­£è¦åŒ–å¯¾å¿œ
**å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«**: `app/services/normalize.py`
**ç¾åœ¨ã®å®Ÿè£…**: BRK.Bâ†’BRK-Bç­‰ã®å¤‰æ›ã€å–å¼•æ‰€ã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹å¯¾å¿œ

**å®Ÿè£…å†…å®¹**:
```python
# app/services/normalize.py ã® normalize_symbolé–¢æ•°ã‚’æ›´æ–°
def normalize_symbol(symbol: Optional[str]) -> str:
    """Normalize ticker symbols to Yahoo Finance style.
    
    ç¾åœ¨ã®ä»•æ§˜:
    - BRK.B â†’ BRK-B (ã‚¯ãƒ©ã‚¹æ ªå¤‰æ›)
    - å–å¼•æ‰€ã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹ (.TOç­‰) ã¯ç¶­æŒ
    
    è¿½åŠ ä»•æ§˜:
    - ^VIXç­‰ã®æŒ‡æ•°ã‚·ãƒ³ãƒœãƒ«ã¯ãã®ã¾ã¾ç¶­æŒ
    """
    if not symbol:
        return ""
    
    s = symbol.strip().upper()
    if not s:
        return ""
    
    # æ–°è¦è¿½åŠ : ç‰¹æ®Šã‚·ãƒ³ãƒœãƒ«ï¼ˆ^ã§å§‹ã¾ã‚‹æŒ‡æ•°ï¼‰ã¯ãã®ã¾ã¾ç¶­æŒ
    if s.startswith("^"):
        return s
    
    # æ—¢å­˜ã®ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆãƒ‰ãƒƒãƒˆå‡¦ç†ï¼‰
    if "." in s:
        head, tail = s.rsplit(".", 1)
        if tail in _KNOWN_EXCHANGE_SUFFIXES:
            return f"{head}.{tail}"
        if len(tail) == 1 and tail.isalpha():
            return f"{head}-{tail}"
        return s
    
    return s
```

**ãƒ†ã‚¹ãƒˆæ–¹æ³•**:
```python
from app.services.normalize import normalize_symbol

# æ–°è¦ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹
assert normalize_symbol("^vix") == "^VIX"
assert normalize_symbol("^GSPC") == "^GSPC"
assert normalize_symbol("^DJI") == "^DJI"

# æ—¢å­˜æ©Ÿèƒ½ãŒå£Šã‚Œã¦ã„ãªã„ã“ã¨ã‚’ç¢ºèª
assert normalize_symbol("BRK.B") == "BRK-B"
assert normalize_symbol("TSM.TW") == "TSM.TW"
```

**å®Œäº†æ¡ä»¶**: ^VIXã‚’å«ã‚€10éŠ˜æŸ„ãŒæ­£ã—ãæ­£è¦åŒ–ã•ã‚Œã‚‹

### Phase 4: ä¸¦è¡Œå–å¾—ã®å®Ÿè£…ï¼ˆä¾å­˜: Phase 1ï¼‰

#### TASK-006: ãƒãƒƒãƒå–å¾—é–¢æ•°ã®å®Ÿè£… â¬œ
**è²¬ä»»**: è¤‡æ•°éŠ˜æŸ„ã®ä¸¦è¡Œå–å¾—
**å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«**: `app/services/fetcher.py`
**ç¾åœ¨ã®å®Ÿè£…**: fetch_pricesé–¢æ•°ï¼ˆåŒæœŸçš„ã«yfinanceã‚’å‘¼ã³å‡ºã—ï¼‰
**å¿…è¦ãªã‚¤ãƒ³ãƒãƒ¼ãƒˆ**: `from starlette.concurrency import run_in_threadpool`

**å®Ÿè£…å†…å®¹**:
```python
# app/services/fetcher.py ã«ä»¥ä¸‹ã®é–¢æ•°ã‚’è¿½åŠ 
import asyncio
from typing import Dict, List, Tuple, Optional
from starlette.concurrency import run_in_threadpool

async def fetch_prices_batch(
    symbols: List[str],
    start: date,
    end: date,
    settings: Settings
) -> Dict[str, pd.DataFrame]:
    """
    è¤‡æ•°éŠ˜æŸ„ã‚’ä¸¦è¡Œå–å¾—ã™ã‚‹æ–°è¦é–¢æ•°
    
    Parameters:
    -----------
    symbols: éŠ˜æŸ„ãƒªã‚¹ãƒˆï¼ˆä¾‹: ["AAPL", "MSFT", "^VIX"]ï¼‰
    start: é–‹å§‹æ—¥
    end: çµ‚äº†æ—¥
    settings: ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®šï¼ˆYF_REQ_CONCURRENCYç­‰ã‚’å«ã‚€ï¼‰
    
    Returns:
    --------
    Dict[str, pd.DataFrame]: éŠ˜æŸ„åã‚’ã‚­ãƒ¼ã€DataFrameã‚’å€¤ã¨ã™ã‚‹è¾æ›¸
    """
    
    async def fetch_one(symbol: str) -> Tuple[str, Optional[pd.DataFrame]]:
        """å˜ä¸€éŠ˜æŸ„ã‚’éåŒæœŸã§å–å¾—"""
        try:
            # æ—¢å­˜ã®fetch_pricesé–¢æ•°ã‚’åˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã§å®Ÿè¡Œ
            df = await run_in_threadpool(
                fetch_prices, 
                symbol, 
                start, 
                end, 
                settings=settings
            )
            return symbol, df
        except Exception as e:
            logger = logging.getLogger(__name__)
            logger.warning(f"Failed to fetch {symbol}: {e}")
            return symbol, None
    
    # ã‚»ãƒãƒ•ã‚©ã§åŒæ™‚æ¥ç¶šæ•°ã‚’åˆ¶å¾¡ï¼ˆYF_REQ_CONCURRENCYã®å€¤ã‚’ä½¿ç”¨ï¼‰
    semaphore = asyncio.Semaphore(settings.YF_REQ_CONCURRENCY)
    
    async def fetch_with_semaphore(symbol: str):
        """ã‚»ãƒãƒ•ã‚©ã§ä¸¦è¡Œæ•°ã‚’åˆ¶é™ã—ãªãŒã‚‰å–å¾—"""
        async with semaphore:
            return await fetch_one(symbol)
    
    # å…¨éŠ˜æŸ„ã‚’ä¸¦è¡Œå‡¦ç†
    tasks = [fetch_with_semaphore(s) for s in symbols]
    results = await asyncio.gather(*tasks)
    
    # æˆåŠŸã—ãŸã‚‚ã®ã ã‘è¾æ›¸ã«æ ¼ç´
    return {symbol: df for symbol, df in results if df is not None and not df.empty}
```

**ãƒ†ã‚¹ãƒˆæ–¹æ³•**:
```python
# tests/test_batch_fetch.py
import asyncio
from datetime import date, timedelta
from app.services.fetcher import fetch_prices_batch
from app.core.config import settings

async def test_batch():
    symbols = ["AAPL", "MSFT", "GOOGL"]
    end = date.today()
    start = end - timedelta(days=7)
    
    results = await fetch_prices_batch(symbols, start, end, settings)
    assert len(results) > 0
    assert "AAPL" in results
    
# å®Ÿè¡Œ
asyncio.run(test_batch())
```

**å®Œäº†æ¡ä»¶**: 3éŠ˜æŸ„ã‚’ä¸¦è¡Œå–å¾—ã—ã€é€æ¬¡ã‚ˆã‚Šé«˜é€Ÿ

#### TASK-007: ensure_coverage ã®ä¸¦è¡ŒåŒ– â¬œ
**è²¬ä»»**: ã‚«ãƒãƒ¬ãƒƒã‚¸ç¢ºèªã¨å–å¾—ã®ä¸¦è¡Œå‡¦ç†
**å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«**: `app/db/queries.py`
**ç¾åœ¨ã®å®Ÿè£…**: ensure_coverageé–¢æ•°ï¼ˆforãƒ«ãƒ¼ãƒ—ã§é€æ¬¡å‡¦ç†ï¼‰
**æ³¨æ„ç‚¹**: æ—¢å­˜ã®with_symbol_lock, _ensure_full_history_once, _get_coverageç­‰ã‚’åˆ©ç”¨

**å®Ÿè£…å†…å®¹**:
```python
# app/db/queries.py ã«ä»¥ä¸‹ã®é–¢æ•°ã‚’è¿½åŠ 
import asyncio
from typing import Sequence
from datetime import date

async def ensure_coverage_parallel(
    session: AsyncSession,
    symbols: Sequence[str],
    date_from: date,
    date_to: date,
    refetch_days: int,
) -> None:
    """
    è¤‡æ•°éŠ˜æŸ„ã®ã‚«ãƒãƒ¬ãƒƒã‚¸ã‚’ä¸¦è¡Œç¢ºèªãƒ»å–å¾—ã™ã‚‹æ–°è¦é–¢æ•°
    æ—¢å­˜ã®ensure_coverageé–¢æ•°ã®ä¸¦è¡Œç‰ˆ
    
    Parameters:
    -----------
    session: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚»ãƒƒã‚·ãƒ§ãƒ³
    symbols: éŠ˜æŸ„ãƒªã‚¹ãƒˆ
    date_from: é–‹å§‹æ—¥
    date_to: çµ‚äº†æ—¥  
    refetch_days: å†å–å¾—æ—¥æ•°ï¼ˆæ—¢å®š30æ—¥ï¼‰
    """
    logger = logging.getLogger(__name__)
    
    async def process_single_symbol(symbol: str):
        """å˜ä¸€éŠ˜æŸ„ã®å‡¦ç†ï¼ˆæ—¢å­˜ã®ensure_coverageã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’åˆ©ç”¨ï¼‰"""
        try:
            # ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒªãƒ­ãƒƒã‚¯å–å¾—
            await with_symbol_lock(session, symbol)
            
            # ä¸€åº¦ã ã‘ãƒ•ãƒ«å±¥æ­´ã‚’ç¢ºä¿
            await _ensure_full_history_once(session, symbol)
            
            # ã‚«ãƒãƒ¬ãƒƒã‚¸ç¢ºèª
            cov = await _get_coverage(session, symbol, date_from, date_to)
            
            last_date = cov.get("last_date")
            first_date = cov.get("first_date") 
            has_gaps = bool(cov.get("has_weekday_gaps") or cov.get("has_gaps"))
            first_missing_weekday = cov.get("first_missing_weekday")
            
            # å–å¾—ç¯„å›²ã®æ±ºå®šï¼ˆæ—¢å­˜ã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’ä½¿ç”¨ï¼‰
            fetch_ranges = []
            
            # æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã®å†å–å¾—
            if last_date and date_to >= last_date:
                days_since_last = (date_to - last_date).days
                if days_since_last > 1:
                    refetch_start = max(date_from, last_date - timedelta(days=refetch_days))
                    if refetch_start <= date_to:
                        fetch_ranges.append((refetch_start, date_to))
            
            # ã‚®ãƒ£ãƒƒãƒ—ã®åŸ‹ã‚è¾¼ã¿
            if has_gaps and first_missing_weekday:
                gap_end = first_date if first_date else date_to
                gap_start = max(date_from, first_missing_weekday)
                if gap_start < gap_end:
                    fetch_ranges.append((gap_start, min(gap_end, date_to)))
            
            # åˆæœŸãƒ‡ãƒ¼ã‚¿
            if not first_date:
                fetch_ranges.append((date_from, date_to))
            
            if not fetch_ranges:
                logger.debug(f"No fetch needed for {symbol}")
                return
            
            # ç¯„å›²ã‚’ãƒãƒ¼ã‚¸
            fetch_ranges.sort()
            merged_ranges = [fetch_ranges[0]] if fetch_ranges else []
            for start, end in fetch_ranges[1:]:
                last_start, last_end = merged_ranges[-1]
                if start <= last_end + timedelta(days=1):
                    merged_ranges[-1] = (last_start, max(last_end, end))
                else:
                    merged_ranges.append((start, end))
            
            # ãƒ‡ãƒ¼ã‚¿å–å¾—ã¨UPSERTï¼ˆæ—¢å­˜ã®é–¢æ•°ã‚’åˆ©ç”¨ï¼‰
            for start, end in merged_ranges:
                df = await fetch_prices_df(symbol, start, end)
                if df is None or df.empty:
                    continue
                    
                rows = df_to_rows(df, symbol=symbol, source="yfinance")
                if not rows:
                    continue
                    
                up_sql = text(upsert_prices_sql())
                await session.execute(up_sql, rows)
                logger.debug(f"Upserted {len(rows)} rows for {symbol}")
                
        except Exception as e:
            logger.error(f"Error processing {symbol}: {e}", exc_info=True)
    
    # ä¸¦è¡Œå‡¦ç†ï¼ˆæœ€å¤§10éŠ˜æŸ„ãšã¤ï¼‰
    chunk_size = min(10, settings.YF_REQ_CONCURRENCY)
    for i in range(0, len(symbols), chunk_size):
        chunk = symbols[i:i+chunk_size]
        tasks = [process_single_symbol(s) for s in chunk]
        await asyncio.gather(*tasks, return_exceptions=True)
```

**æ—¢å­˜é–¢æ•°ã¨ã®é–¢ä¿‚**:
- `ensure_coverage`: ãã®ã¾ã¾æ®‹ã™ï¼ˆå¾Œæ–¹äº’æ›æ€§ï¼‰
- `ensure_coverage_parallel`: æ–°è¦è¿½åŠ ï¼ˆä¸¦è¡Œç‰ˆï¼‰
- å†…éƒ¨ã§ä½¿ç”¨: `with_symbol_lock`, `_ensure_full_history_once`, `_get_coverage`, `fetch_prices_df`, `df_to_rows`, `upsert_prices_sql`

**ãƒ†ã‚¹ãƒˆæ–¹æ³•**:
```python
# 5éŠ˜æŸ„ã§ã®ãƒ†ã‚¹ãƒˆ
symbols = ["AAPL", "MSFT", "GOOGL", "TSLA", "AMZN"]
await ensure_coverage_parallel(session, symbols, date_from, date_to, 30)
```

**å®Œäº†æ¡ä»¶**: 5éŠ˜æŸ„ã‚’ä¸¦è¡Œå‡¦ç†ã—ã¦ã‚¨ãƒ©ãƒ¼ãªã—

### Phase 5: ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒã‚µãƒ¼ãƒ“ã‚¹ã®å®Ÿè£…ï¼ˆä¾å­˜: Phase 2, 3, 4ï¼‰

#### TASK-008: ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒã‚µãƒ¼ãƒ“ã‚¹ã‚¯ãƒ©ã‚¹å®Ÿè£… â¬œ
**è²¬ä»»**: æŒ‡å®šéŠ˜æŸ„ã®äº‹å‰å–å¾—ã¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†
**å…¥åŠ›**: TASK-003, TASK-004, TASK-006
**å‡ºåŠ›**: app/services/prefetch_service.py
**å®Ÿè£…**:
```python
# app/services/prefetch_service.py
import asyncio
from datetime import date, datetime, timedelta
from typing import List, Optional, Dict, Any
import logging

from app.core.config import settings
from app.services.cache import get_cache
from app.services.fetcher import fetch_prices_batch
from app.db.engine import create_engine_and_sessionmaker
from app.db.queries import ensure_coverage_parallel

logger = logging.getLogger(__name__)

class PrefetchService:
    def __init__(self):
        self.symbols = self._parse_symbols()
        self.cache = get_cache()
        self.running = False
        self._task: Optional[asyncio.Task] = None
    
    def _parse_symbols(self) -> List[str]:
        """ç’°å¢ƒå¤‰æ•°ã‹ã‚‰éŠ˜æŸ„ãƒªã‚¹ãƒˆã‚’å–å¾—"""
        symbols_str = settings.PREFETCH_SYMBOLS
        if not symbols_str:
            return []
        return [s.strip() for s in symbols_str.split(",") if s.strip()]
    
    async def start(self):
        """ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒã‚µãƒ¼ãƒ“ã‚¹é–‹å§‹"""
        if self.running:
            return
        
        self.running = True
        logger.info(f"Starting prefetch for {len(self.symbols)} symbols")
        
        # åˆå›å–å¾—
        await self._prefetch_all()
        
        # å®šæœŸæ›´æ–°ã‚¿ã‚¹ã‚¯é–‹å§‹
        self._task = asyncio.create_task(self._periodic_update())
    
    async def stop(self):
        """ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒã‚µãƒ¼ãƒ“ã‚¹åœæ­¢"""
        self.running = False
        if self._task:
            self._task.cancel()
            try:
                await self._task
            except asyncio.CancelledError:
                pass
    
    async def _prefetch_all(self):
        """å…¨éŠ˜æŸ„ã‚’ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒ"""
        if not self.symbols:
            return
        
        try:
            # ç‹¬ç«‹ã—ãŸDBã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œæˆ
            _, SessionLocal = create_engine_and_sessionmaker(
                database_url=settings.DATABASE_URL,
                pool_size=2
            )
            
            async with SessionLocal() as session:
                # æœ€æ–°30æ—¥åˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºä¿
                today = date.today()
                from_date = today - timedelta(days=30)
                
                # ä¸¦è¡Œå–å¾—
                await ensure_coverage_parallel(
                    session, self.symbols, from_date, today,
                    settings.YF_REFETCH_DAYS
                )
                
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
                for symbol in self.symbols:
                    cache_key = f"prefetch:{symbol}:{from_date}:{today}"
                    await self.cache.set(cache_key, True)
                
            logger.info(f"Prefetched {len(self.symbols)} symbols")
            
        except Exception as e:
            logger.error(f"Prefetch failed: {e}")
    
    async def _periodic_update(self):
        """å®šæœŸçš„ãªæ›´æ–°"""
        interval = settings.PREFETCH_INTERVAL_MINUTES * 60
        
        while self.running:
            try:
                await asyncio.sleep(interval)
                await self._prefetch_all()
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"Periodic update failed: {e}")

# ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
_prefetch_service: Optional[PrefetchService] = None

def get_prefetch_service() -> PrefetchService:
    global _prefetch_service
    if _prefetch_service is None:
        _prefetch_service = PrefetchService()
    return _prefetch_service
```
**ãƒ†ã‚¹ãƒˆ**: start/stop/ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒå®Ÿè¡Œã®ç¢ºèª
**å®Œäº†æ¡ä»¶**: æŒ‡å®šéŠ˜æŸ„ãŒãƒ—ãƒªãƒ•ã‚§ãƒƒãƒã•ã‚Œã‚‹

#### TASK-009: ã‚¢ãƒ—ãƒªèµ·å‹•æ™‚ã®ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒé–‹å§‹ â¬œ
**è²¬ä»»**: FastAPIèµ·å‹•æ™‚ã«ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒã‚’è‡ªå‹•é–‹å§‹
**å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«**: `app/main.py`
**ç¾åœ¨ã®å®Ÿè£…**: lifespanã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚ã‚Šï¼ˆç©ºå®Ÿè£…ï¼‰
**æ³¨æ„ç‚¹**: æ—¢å­˜ã®importæ–‡ã¨lifespanã®æ§‹é€ ã‚’ç¶­æŒ

**å®Ÿè£…å†…å®¹**:
```python
# app/main.py ã®lifespanã‚’æ›´æ–°
import asyncio
import logging
from contextlib import asynccontextmanager
from fastapi import FastAPI
from app.core.config import settings

logger = logging.getLogger(__name__)

@asynccontextmanager
async def lifespan(_: FastAPI):
    """ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ç®¡ç†"""
    prefetch_service = None
    try:
        # èµ·å‹•æ™‚ã®å‡¦ç†
        logger.info("Starting application...")
        
        # ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒã‚µãƒ¼ãƒ“ã‚¹é–‹å§‹ï¼ˆENABLE_CACHEãŒTrueã®å ´åˆã®ã¿ï¼‰
        if settings.ENABLE_CACHE:
            try:
                from app.services.prefetch_service import get_prefetch_service
                prefetch_service = get_prefetch_service()
                await prefetch_service.start()
                logger.info(f"Prefetch service started for {len(prefetch_service.symbols)} symbols")
            except ImportError:
                logger.warning("Prefetch service not found, skipping...")
            except Exception as e:
                logger.error(f"Failed to start prefetch service: {e}")
                # ã‚¨ãƒ©ãƒ¼ãŒèµ·ãã¦ã‚‚ã‚¢ãƒ—ãƒªã¯èµ·å‹•ã•ã›ã‚‹
        
        yield
        
    finally:
        # ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³æ™‚ã®å‡¦ç†
        logger.info("Shutting down application...")
        
        # ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒã‚µãƒ¼ãƒ“ã‚¹åœæ­¢
        if prefetch_service and settings.ENABLE_CACHE:
            try:
                await prefetch_service.stop()
                logger.info("Prefetch service stopped")
            except Exception as e:
                logger.error(f"Error stopping prefetch service: {e}")

# æ—¢å­˜ã®FastAPIã‚¢ãƒ—ãƒªä½œæˆï¼ˆlifespanã‚’æŒ‡å®šï¼‰
app = FastAPI(lifespan=lifespan)

# ä»¥é™ã®æ—¢å­˜ã‚³ãƒ¼ãƒ‰ï¼ˆinit_error_handlers, configure_loggingç­‰ï¼‰ã¯ãã®ã¾ã¾
```

**ç¢ºèªäº‹é …**:
- settings.ENABLE_CACHEãŒFalseã®å ´åˆã¯ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒã‚’ã‚¹ã‚­ãƒƒãƒ—
- ImportErrorã‚’ã‚­ãƒ£ãƒƒãƒã—ã¦ã€ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒã‚µãƒ¼ãƒ“ã‚¹ãŒãªãã¦ã‚‚èµ·å‹•å¯èƒ½
- ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å…¨ä½“ã¯èµ·å‹•ã™ã‚‹

**ãƒ†ã‚¹ãƒˆæ–¹æ³•**:
```bash
# ENABLE_CACHE=true ã§èµ·å‹•
ENABLE_CACHE=true uvicorn app.main:app --reload

# ãƒ­ã‚°ã§ç¢ºèª
# "Prefetch service started for 10 symbols" ãŒè¡¨ç¤ºã•ã‚Œã‚‹ã“ã¨

# ENABLE_CACHE=false ã§èµ·å‹•ã—ã¦ã‚‚ã‚¨ãƒ©ãƒ¼ãªã—
ENABLE_CACHE=false uvicorn app.main:app --reload
```

**å®Œäº†æ¡ä»¶**: 
- ã‚¢ãƒ—ãƒªèµ·å‹•æ™‚ã«"Prefetch service started"ãƒ­ã‚°ãŒå‡ºåŠ›
- ENABLE_CACHE=falseã§ã‚‚æ­£å¸¸èµ·å‹•

### Phase 6: APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®æœ€é©åŒ–ï¼ˆä¾å­˜: Phase 4, 5ï¼‰

#### TASK-010: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯ä»˜ãä¾¡æ ¼å–å¾— â¬œ
**è²¬ä»»**: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç¢ºèªã—ã¦ã‹ã‚‰å–å¾—ã™ã‚‹é«˜é€ŸåŒ–
**å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«**: `app/api/v1/prices.py`
**ç¾åœ¨ã®å®Ÿè£…**: get_pricesé–¢æ•°ï¼ˆensure_coverage â†’ get_prices_resolvedï¼‰
**ä¾å­˜**: TASK-003ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰, TASK-007ï¼ˆä¸¦è¡ŒåŒ–ï¼‰

**å®Ÿè£…å†…å®¹**:
```python
# app/api/v1/prices.py ã®get_pricesé–¢æ•°ã‚’æ›´æ–°
import time
from typing import List
from datetime import date
from fastapi import Query, Depends, HTTPException
from app.api.deps import get_session
from app.core.config import settings
from app.schemas.prices import PriceRowOut

@router.get("/prices", response_model=List[PriceRowOut])
async def get_prices(
    symbols: str = Query(..., description="Comma-separated symbols"),
    date_from: date = Query(..., alias="from"),
    date_to: date = Query(..., alias="to"),
    auto_fetch: bool = Query(True, description="Auto-fetch all available data if missing"),
    session=Depends(get_session),
):
    # æ—¢å­˜ã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
    if date_to < date_from:
        raise HTTPException(status_code=422, detail="invalid date range")
    
    symbols_list = _parse_and_validate_symbols(symbols)
    if not symbols_list:
        return []
    
    # è‡ªå‹•ç™»éŒ²ï¼ˆæ—¢å­˜ã®ã‚³ãƒ¼ãƒ‰ï¼‰
    if settings.ENABLE_AUTO_REGISTRATION:
        logger.info(f"Checking auto-registration for symbols: {symbols_list}")
        await ensure_symbols_registered(session, symbols_list)
    
    t0 = time.perf_counter()
    effective_to = min(date_to, date.today())
    
    # === ã“ã“ã‹ã‚‰æ–°è¦è¿½åŠ  ===
    cached_results = []
    uncached_symbols = []
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯ï¼ˆENABLE_CACHEãŒTrueã®å ´åˆã®ã¿ï¼‰
    if settings.ENABLE_CACHE:
        try:
            from app.services.cache import get_cache
            cache = get_cache()
            
            for symbol in symbols_list:
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã®ç”Ÿæˆï¼ˆã‚·ãƒ³ãƒœãƒ«ã€æœŸé–“ã§ä¸€æ„ï¼‰
                cache_key = f"prices:{symbol}:{date_from}:{effective_to}"
                cached_data = await cache.get(cache_key)
                
                if cached_data:
                    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ
                    cached_results.extend(cached_data)
                    logger.debug(f"Cache hit for {symbol}")
                else:
                    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹
                    uncached_symbols.append(symbol)
                    logger.debug(f"Cache miss for {symbol}")
            
            # å…¨ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚ã‚Œã°å³åº§ã«è¿”å´
            if not uncached_symbols:
                logger.info(f"All {len(symbols_list)} symbols from cache")
                return cached_results
                
        except ImportError:
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒãªã„å ´åˆã¯å…¨ã¦å–å¾—
            uncached_symbols = symbols_list
        except Exception as e:
            logger.warning(f"Cache check failed: {e}")
            uncached_symbols = symbols_list
    else:
        uncached_symbols = symbols_list
    
    # === ä¸¦è¡Œå‡¦ç†ç‰ˆã®ä½¿ç”¨ï¼ˆTASK-007ã®æˆæœã‚’åˆ©ç”¨ï¼‰ ===
    if auto_fetch and uncached_symbols:
        # ensure_coverage_parallelãŒå­˜åœ¨ã™ã‚Œã°ä½¿ç”¨ã€ãªã‘ã‚Œã°é€šå¸¸ç‰ˆ
        try:
            from app.db.queries import ensure_coverage_parallel
            await ensure_coverage_parallel(
                session=session,
                symbols=uncached_symbols,
                date_from=date_from,
                date_to=effective_to,
                refetch_days=settings.YF_REFETCH_DAYS,
            )
            logger.info(f"Used parallel coverage for {len(uncached_symbols)} symbols")
        except ImportError:
            # ä¸¦è¡Œç‰ˆãŒãªã‘ã‚Œã°æ—¢å­˜ã®é€æ¬¡ç‰ˆã‚’ä½¿ç”¨
            await queries.ensure_coverage(
                session=session,
                symbols=uncached_symbols,
                date_from=date_from,
                date_to=effective_to,
                refetch_days=settings.YF_REFETCH_DAYS,
            )
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰å–å¾—
    rows = []
    for symbol in uncached_symbols:
        symbol_rows = await queries.get_prices_resolved(
            session=session,
            symbols=[symbol],
            date_from=date_from,
            date_to=effective_to,
        )
        rows.extend(symbol_rows)
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜ï¼ˆENABLE_CACHEãŒTrueã®å ´åˆï¼‰
        if settings.ENABLE_CACHE and symbol_rows:
            try:
                cache_key = f"prices:{symbol}:{date_from}:{effective_to}"
                await cache.set(cache_key, symbol_rows)
                logger.debug(f"Cached {len(symbol_rows)} rows for {symbol}")
            except Exception as e:
                logger.warning(f"Failed to cache {symbol}: {e}")
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ¸ˆã¿ã¨æ–°è¦å–å¾—ã‚’çµåˆ
    if cached_results:
        rows.extend(cached_results)
    
    # ã‚½ãƒ¼ãƒˆï¼ˆæ—¥ä»˜ã€ã‚·ãƒ³ãƒœãƒ«é †ï¼‰
    rows.sort(key=lambda r: (r["date"], r["symbol"]))
    
    # è¡Œæ•°åˆ¶é™ãƒã‚§ãƒƒã‚¯
    if len(rows) > settings.API_MAX_ROWS:
        raise HTTPException(status_code=413, detail="response too large")
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ­ã‚°
    dt_ms = int((time.perf_counter() - t0) * 1000)
    cache_hit_count = len(cached_results) if settings.ENABLE_CACHE else 0
    
    logger.info(
        "prices served",
        extra=dict(
            symbols=symbols_list,
            date_from=str(date_from),
            date_to=str(effective_to),
            rows=len(rows),
            duration_ms=dt_ms,
            cache_hits=cache_hit_count,
            cache_hit_ratio=cache_hit_count/len(symbols_list) if symbols_list else 0,
        ),
    )
    
    return rows
```

**é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ**:
1. ENABLE_CACHE=falseã§ã‚‚å‹•ä½œï¼ˆæ—¢å­˜ã®å‡¦ç†ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
2. ImportErrorã‚’ã‚­ãƒ£ãƒƒãƒï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒãªãã¦ã‚‚å‹•ä½œï¼‰
3. ä¸¦è¡Œç‰ˆensure_coverage_parallelã‚’å„ªå…ˆä½¿ç”¨
4. ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡ã‚’ãƒ­ã‚°ã«è¨˜éŒ²

**ãƒ†ã‚¹ãƒˆæ–¹æ³•**:
```bash
# åˆå›ãƒªã‚¯ã‚¨ã‚¹ãƒˆï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹ï¼‰
curl "http://localhost:8000/v1/prices?symbols=AAPL,MSFT&from=2024-01-01&to=2024-01-31"

# 2å›ç›®ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆã€é«˜é€Ÿï¼‰
curl "http://localhost:8000/v1/prices?symbols=AAPL,MSFT&from=2024-01-01&to=2024-01-31"
```

**å®Œäº†æ¡ä»¶**: 2å›ç›®ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒåˆå›ã‚ˆã‚Šå¤§å¹…ã«é«˜é€Ÿ

### Phase 7: ãƒ†ã‚¹ãƒˆã®å®Ÿè£…ï¼ˆä¾å­˜: Phase 1-6ï¼‰

#### TASK-011: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆ â¬œ
**è²¬ä»»**: InMemoryCacheã‚¯ãƒ©ã‚¹ã®ãƒ†ã‚¹ãƒˆ
**å…¥åŠ›**: TASK-003
**å‡ºåŠ›**: tests/unit/test_cache.py
**å®Ÿè£…**:
```python
# tests/unit/test_cache.py
import pytest
import asyncio
from datetime import datetime, timedelta
from app.services.cache import InMemoryCache

@pytest.mark.asyncio
async def test_cache_basic():
    cache = InMemoryCache(ttl_seconds=1)
    
    # Set and get
    await cache.set("key1", "value1")
    assert await cache.get("key1") == "value1"
    
    # TTL expiration
    await asyncio.sleep(1.1)
    assert await cache.get("key1") is None

@pytest.mark.asyncio
async def test_cache_max_size():
    cache = InMemoryCache(ttl_seconds=60, max_size=2)
    
    await cache.set("key1", "value1")
    await cache.set("key2", "value2")
    await cache.set("key3", "value3")  # key1 should be evicted
    
    assert await cache.get("key1") is None
    assert await cache.get("key2") == "value2"
    assert await cache.get("key3") == "value3"
```
**ãƒ†ã‚¹ãƒˆ**: pytest tests/unit/test_cache.py
**å®Œäº†æ¡ä»¶**: å…¨ãƒ†ã‚¹ãƒˆåˆæ ¼

#### TASK-012: ä¸¦è¡Œå–å¾—ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ â¬œ
**è²¬ä»»**: ä¸¦è¡Œå–å¾—ã®é€Ÿåº¦æ¸¬å®š
**å…¥åŠ›**: TASK-006
**å‡ºåŠ›**: tests/performance/test_parallel_fetch.py
**å®Ÿè£…**:
```python
# tests/performance/test_parallel_fetch.py
import pytest
import asyncio
import time
from app.services.fetcher import fetch_prices_batch, fetch_prices
from app.core.config import settings
from datetime import date, timedelta

@pytest.mark.asyncio
async def test_parallel_vs_sequential():
    symbols = ["AAPL", "MSFT", "GOOGL"]
    end = date.today()
    start = end - timedelta(days=30)
    
    # Sequential
    t0 = time.time()
    for symbol in symbols:
        await asyncio.to_thread(fetch_prices, symbol, start, end, settings=settings)
    seq_time = time.time() - t0
    
    # Parallel
    t0 = time.time()
    await fetch_prices_batch(symbols, start, end, settings)
    par_time = time.time() - t0
    
    print(f"Sequential: {seq_time:.2f}s, Parallel: {par_time:.2f}s")
    assert par_time < seq_time * 0.7  # ä¸¦è¡ŒãŒ30%ä»¥ä¸Šé«˜é€Ÿ
```
**ãƒ†ã‚¹ãƒˆ**: pytest tests/performance/test_parallel_fetch.py -s
**å®Œäº†æ¡ä»¶**: ä¸¦è¡Œå‡¦ç†ãŒé«˜é€Ÿ

#### TASK-013: ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒã‚µãƒ¼ãƒ“ã‚¹ã®çµ±åˆãƒ†ã‚¹ãƒˆ â¬œ
**è²¬ä»»**: ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒæ©Ÿèƒ½ã®å‹•ä½œç¢ºèª
**å…¥åŠ›**: TASK-008
**å‡ºåŠ›**: tests/integration/test_prefetch.py
**å®Ÿè£…**:
```python
# tests/integration/test_prefetch.py
import pytest
import asyncio
from app.services.prefetch_service import PrefetchService
from app.services.cache import InMemoryCache

@pytest.mark.asyncio
async def test_prefetch_service():
    # ãƒ†ã‚¹ãƒˆç”¨è¨­å®š
    service = PrefetchService()
    service.symbols = ["AAPL", "MSFT"]  # ãƒ†ã‚¹ãƒˆç”¨ã«2éŠ˜æŸ„ã®ã¿
    
    # ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒå®Ÿè¡Œ
    await service._prefetch_all()
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç¢ºèª
    cache = service.cache
    from datetime import date, timedelta
    today = date.today()
    from_date = today - timedelta(days=30)
    
    for symbol in service.symbols:
        cache_key = f"prefetch:{symbol}:{from_date}:{today}"
        assert await cache.get(cache_key) is not None
```
**ãƒ†ã‚¹ãƒˆ**: pytest tests/integration/test_prefetch.py
**å®Œäº†æ¡ä»¶**: ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒå‹•ä½œç¢ºèª

### Phase 8: ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã¨ãƒ‡ãƒãƒƒã‚°ï¼ˆä¾å­˜: Phase 1-7ï¼‰

#### TASK-014: ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ â¬œ
**è²¬ä»»**: ã‚­ãƒ£ãƒƒã‚·ãƒ¥åˆ©ç”¨çŠ¶æ³ã®å¯è¦–åŒ–
**å…¥åŠ›**: TASK-003
**å‡ºåŠ›**: app/api/v1/debug.py
**å®Ÿè£…**:
```python
# app/api/v1/debug.py
from fastapi import APIRouter, Depends, HTTPException
from app.core.config import settings
from app.api.deps import get_settings

router = APIRouter()

@router.get("/debug/cache-stats")
async def get_cache_stats(settings=Depends(get_settings)):
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆæƒ…å ±ï¼ˆé–‹ç™ºç’°å¢ƒã®ã¿ï¼‰"""
    if settings.APP_ENV not in ["development", "staging"]:
        raise HTTPException(status_code=404)
    
    from app.services.cache import get_cache
    cache = get_cache()
    
    with cache._lock:
        total_items = len(cache._cache)
        items_info = []
        for key, (value, timestamp) in list(cache._cache.items())[:10]:
            items_info.append({
                "key": key,
                "age_seconds": (datetime.utcnow() - timestamp).total_seconds(),
                "size_bytes": len(str(value))
            })
    
    return {
        "total_items": total_items,
        "max_size": cache._max_size,
        "ttl_seconds": cache._ttl,
        "sample_items": items_info
    }
```
**ãƒ†ã‚¹ãƒˆ**: ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆå‘¼ã³å‡ºã—
**å®Œäº†æ¡ä»¶**: çµ±è¨ˆæƒ…å ±ãŒå–å¾—å¯èƒ½

#### TASK-015: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ­ã‚°ã®è¿½åŠ  â¬œ
**è²¬ä»»**: é«˜é€ŸåŒ–åŠ¹æœã®æ¸¬å®š
**å…¥åŠ›**: TASK-010
**å‡ºåŠ›**: app/api/v1/prices.py ã®æ›´æ–°
**å®Ÿè£…**: ãƒ­ã‚°ã« cache_hits, parallel_fetch_time ç­‰ã‚’è¿½åŠ 
**ãƒ†ã‚¹ãƒˆ**: ãƒ­ã‚°å‡ºåŠ›ç¢ºèª
**å®Œäº†æ¡ä»¶**: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ãŒãƒ­ã‚°ã«è¨˜éŒ²

## å®Ÿè£…é †åºã¨å„ªå…ˆåº¦

### ğŸš€ å³åº§ã«åŠ¹æœãŒå‡ºã‚‹ã‚¿ã‚¹ã‚¯ï¼ˆè¨­å®šå¤‰æ›´ã®ã¿ï¼‰
1. **TASK-001**: ç’°å¢ƒå¤‰æ•°ã®æ›´æ–°ï¼ˆ5åˆ†ï¼‰
2. **TASK-002**: render.yamlã®æ›´æ–°ï¼ˆ5åˆ†ï¼‰

### ğŸ¯ æœ€å¤§åŠ¹æœãŒæœŸå¾…ã§ãã‚‹ã‚¿ã‚¹ã‚¯ï¼ˆä¸¦è¡Œå‡¦ç†ï¼‰
3. **TASK-006**: ãƒãƒƒãƒå–å¾—é–¢æ•°ï¼ˆ30åˆ†ï¼‰
4. **TASK-007**: ensure_coverageã®ä¸¦è¡ŒåŒ–ï¼ˆ45åˆ†ï¼‰

### ğŸ“¦ åŸºç›¤å®Ÿè£…ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¨ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒï¼‰
5. **TASK-003**: ã‚¤ãƒ³ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆ30åˆ†ï¼‰
6. **TASK-004**: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ï¼ˆ10åˆ†ï¼‰
7. **TASK-005**: æ­£è¦åŒ–ã®å¼·åŒ–ï¼ˆ15åˆ†ï¼‰
8. **TASK-008**: ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒã‚µãƒ¼ãƒ“ã‚¹ï¼ˆ45åˆ†ï¼‰
9. **TASK-009**: èµ·å‹•æ™‚ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒï¼ˆ20åˆ†ï¼‰

### ğŸ”§ APIã¨ãƒ†ã‚¹ãƒˆ
10. **TASK-010**: APIæœ€é©åŒ–ï¼ˆ30åˆ†ï¼‰
11. **TASK-011ã€œ015**: ãƒ†ã‚¹ãƒˆã¨ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ï¼ˆå„15-20åˆ†ï¼‰

## æˆåŠŸæŒ‡æ¨™ã¨æ¸¬å®šæ–¹æ³•

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›®æ¨™
- [ ] **10éŠ˜æŸ„åŒæ™‚ãƒªã‚¯ã‚¨ã‚¹ãƒˆ**: 10-15ç§’ â†’ 2-3ç§’ä»¥å†…
- [ ] **ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒ10éŠ˜æŸ„**: 0.1ç§’ä»¥å†…ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å³åº§ã«è¿”å´ï¼‰
- [ ] **ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡**: 60%ä»¥ä¸Šï¼ˆãƒ­ã‚°ã§ç¢ºèªï¼‰

### æ¸¬å®šæ–¹æ³•
```bash
# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®šã‚¹ã‚¯ãƒªãƒ—ãƒˆ
time curl "http://localhost:8000/v1/prices?symbols=TQQQ,TECL,GLD,XLU,^VIX,QQQ,SPY,TMV,TMF,LQD&from=2024-01-01&to=2024-01-31"

# ãƒ­ã‚°ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡ã‚’ç¢ºèª
tail -f logs/app.log | grep "cache_hit_ratio"
```

### ã‚·ã‚¹ãƒ†ãƒ åˆ¶ç´„
- [ ] **ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡**: 500MBä»¥å†…
- [ ] **ã‚¨ãƒ©ãƒ¼ç‡**: 1%æœªæº€
- [ ] **å¾Œæ–¹äº’æ›æ€§**: æ—¢å­˜APIã®å‹•ä½œã‚’å¤‰æ›´ã—ãªã„

## å®Ÿè£…ä¸Šã®é‡è¦ãªæ³¨æ„äº‹é …

### 1. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
- **ImportError**: ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒãªãã¦ã‚‚æ—¢å­˜å‡¦ç†ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
- **ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¨ãƒ©ãƒ¼**: ã‚­ãƒ£ãƒƒã‚·ãƒ¥å¤±æ•—ã—ã¦ã‚‚ãƒ‡ãƒ¼ã‚¿å–å¾—ã¯ç¶™ç¶š
- **ä¸¦è¡Œå‡¦ç†ã‚¨ãƒ©ãƒ¼**: å€‹åˆ¥éŠ˜æŸ„ã®ã‚¨ãƒ©ãƒ¼ãŒå…¨ä½“ã‚’æ­¢ã‚ãªã„

### 2. è¨­å®šã«ã‚ˆã‚‹åˆ¶å¾¡
```python
# ENABLE_CACHE=false ã§å¾“æ¥å‹•ä½œ
if settings.ENABLE_CACHE:
    # æ–°æ©Ÿèƒ½ã‚’ä½¿ç”¨
else:
    # æ—¢å­˜ã®å‡¦ç†ã‚’ä½¿ç”¨
```

### 3. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³
- ä¸¦è¡Œå‡¦ç†æ™‚ã¯å„éŠ˜æŸ„ã§ç‹¬ç«‹ã—ãŸãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³
- ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒªãƒ­ãƒƒã‚¯ã¯ç¶­æŒï¼ˆwith_symbol_lockï¼‰

### 4. ãƒ­ã‚°å‡ºåŠ›
```python
logger.info("Performance metrics", extra={
    "duration_ms": dt_ms,
    "cache_hits": cache_hit_count,
    "parallel_fetch": True
})
```

### 5. ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
```bash
# å˜ä½“ãƒ†ã‚¹ãƒˆ
pytest tests/unit/test_cache.py -v

# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
pytest tests/performance/test_parallel_fetch.py -s

# çµ±åˆãƒ†ã‚¹ãƒˆ
pytest tests/integration/test_prefetch.py -v
```

## å®Ÿè£…ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

### Phase 1: åŸºæœ¬è¨­å®šï¼ˆ10åˆ†ï¼‰
- [ ] TASK-001: config.pyæ›´æ–°
- [ ] TASK-002: render.yamlæ›´æ–°
- [ ] ãƒ‡ãƒ—ãƒ­ã‚¤ç’°å¢ƒã§å‹•ä½œç¢ºèª

### Phase 2: ä¸¦è¡Œå‡¦ç†ï¼ˆ1.5æ™‚é–“ï¼‰
- [ ] TASK-006: fetch_prices_batchå®Ÿè£…
- [ ] TASK-007: ensure_coverage_parallelå®Ÿè£…  
- [ ] ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®š

### Phase 3: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆ1æ™‚é–“ï¼‰
- [ ] TASK-003: InMemoryCacheå®Ÿè£…
- [ ] TASK-004: get_cacheå®Ÿè£…
- [ ] TASK-005: æ­£è¦åŒ–æ›´æ–°

### Phase 4: ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒï¼ˆ1æ™‚é–“ï¼‰
- [ ] TASK-008: PrefetchServiceå®Ÿè£…
- [ ] TASK-009: lifespanæ›´æ–°

### Phase 5: çµ±åˆï¼ˆ30åˆ†ï¼‰
- [ ] TASK-010: get_pricesæœ€é©åŒ–
- [ ] å…¨ä½“å‹•ä½œç¢ºèª

### Phase 6: å“è³ªä¿è¨¼ï¼ˆ1æ™‚é–“ï¼‰
- [ ] TASK-011ã€œ013: ãƒ†ã‚¹ãƒˆä½œæˆ
- [ ] TASK-014ã€œ015: ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°

## ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### Q: ImportError: No module named 'app.services.cache'
A: TASK-003ãŒæœªå®Ÿè£…ã€‚ENABLE_CACHE=falseã§ä¸€æ™‚çš„ã«å›é¿å¯èƒ½

### Q: ä¸¦è¡Œå‡¦ç†ã§ãƒ‡ãƒƒãƒ‰ãƒ­ãƒƒã‚¯
A: chunk_sizeã‚’å°ã•ãã™ã‚‹ï¼ˆ10â†’5ï¼‰

### Q: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ¡ãƒ¢ãƒªä¸è¶³
A: InMemoryCacheã®max_sizeã‚’èª¿æ•´ï¼ˆ1000â†’500ï¼‰

### Q: ^VIXãŒå–å¾—ã§ããªã„
A: TASK-005ã®æ­£è¦åŒ–æ›´æ–°ãŒå¿…è¦

## æœ€çµ‚ç¢ºèªäº‹é …

1. **æ—¢å­˜æ©Ÿèƒ½ã¸ã®å½±éŸ¿ãªã—**: å¾“æ¥ã®APIãŒæ­£å¸¸å‹•ä½œ
2. **æ®µéšçš„ç§»è¡Œå¯èƒ½**: ENABLE_CACHEã§åˆ‡ã‚Šæ›¿ãˆ
3. **ã‚¨ãƒ©ãƒ¼æ™‚ã®å®‰å…¨æ€§**: æ–°æ©Ÿèƒ½ã‚¨ãƒ©ãƒ¼ã§ã‚‚åŸºæœ¬æ©Ÿèƒ½ã¯å‹•ä½œ
4. **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Š**: ç›®æ¨™å€¤ã‚’é”æˆ
5. **é‹ç”¨æ€§**: ãƒ­ã‚°ã¨ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ãŒå……å®Ÿ
