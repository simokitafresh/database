# 株価データ管理基盤 - クリティカルエラー修正プラン（改訂版）

## 📅 作成日: 2025年9月6日
## 🎯 目的: Supabase + Render Starter Plan環境での安定稼働 + 自動データ取得機能

---

## 1. 🔴 最優先: queries_new.py - SQLAlchemy構文エラー

### WHY（なぜ修正が必要か）
- **bindparam構文エラー**により、このモジュールをimportすると即座にAttributeErrorが発生
- queries.pyが正常動作しているが、誤ってqueries_new.pyが呼ばれるとAPI全体がクラッシュ
- 別アプリからAPIを叩いた際に500エラーを返す原因

### WHAT（何を修正するか）
- queries_new.pyの削除またはbindparam構文の修正
- 正常動作しているqueries.pyの維持

### AS-IS（現状）
```python
# app/db/queries_new.py - 行67-73
sql = text(
    """
    WITH rng AS (
        SELECT :date_from::date AS dfrom, :date_to::date AS dto
    ),
    ...
    """
).bindparam(  # ❌ AttributeError: 'TextClause' object has no attribute 'bindparam'
    bindparam("symbol", String),
    bindparam("date_from", Date),
    bindparam("date_to", Date)
)
```

### TO-BE（修正後）
**オプション1: queries_new.pyを削除（推奨）**
```bash
# ファイル削除
rm app/db/queries_new.py
```

**オプション2: 構文修正**
```python
# app/db/queries_new.py - 修正版
from sqlalchemy import bindparam

sql = text(
    """
    WITH rng AS (
        SELECT :date_from::date AS dfrom, :date_to::date AS dto
    ),
    ...
    """
)
# bindparamは別途定義、executeで渡す
res = await session.execute(
    sql, 
    {"symbol": symbol, "date_from": date_from, "date_to": date_to}
)
```

---

## 2. 🔴 Supabase接続最適化

### WHY
- Supabase Poolerモード（pgbouncer）使用時にprepared statement衝突
- 接続プール設定がSupabaseの制限に合っていない
- `statement_cache_size`のデフォルト値100が原因でエラー頻発

### WHAT
- NullPool使用（pgbouncer経由時）
- statement_cache無効化
- 適切な接続数設定

### AS-IS
```python
# app/db/engine.py
def create_engine_and_sessionmaker(
    database_url: str,
    pool_size: int = 5,  # 多すぎる
    max_overflow: int = 5,
    ...
):
    # statement_cache_sizeの設定なし（デフォルト100）
    connect_args = {}
```

### TO-BE
```python
# app/db/engine.py
def create_engine_and_sessionmaker(
    database_url: str,
    pool_size: int = 2,  # Starter Plan向け最適化
    max_overflow: int = 3,
    ...
):
    connect_args = {}
    poolclass = None
    
    if database_url.startswith("postgresql+asyncpg://"):
        # Supabase必須設定
        connect_args["statement_cache_size"] = 0  # 必須
        connect_args["prepared_statement_name_func"] = (
            lambda: f"__asyncpg_{uuid.uuid4()}__"
        )
        
        # Pooler使用判定
        if "pooler.supabase.com" in database_url:
            poolclass = NullPool  # pgbouncer経由は必須
            logger.info("Using NullPool for Supabase Pooler mode")
```

---

## 3. 🔴 entrypoint.sh簡潔化

### WHY
- 400行超の複雑な処理でデバッグ困難
- 環境変数処理の脆弱性
- エラー時の原因特定が困難
- デプロイ失敗の主要因

### WHAT
- 環境変数処理を10行以内に
- 明確なエラーメッセージ
- 不要な互換性処理の削除

### AS-IS
```bash
# docker/entrypoint.sh - 複雑な環境変数処理
if '=' in url and url.startswith(('ALEMBIC_DATABASE_URL=', 'DATABASE_URL=')):
    url = url.split('=', 1)[1]
    print(f'[DEBUG] Cleaned URL after split: {repr(url)}')
# ... 400行以上の処理
```

### TO-BE
```bash
#!/usr/bin/env bash
set -euo pipefail

# 環境変数設定（シンプル）
export ALEMBIC_DATABASE_URL="${ALEMBIC_DATABASE_URL:-${DATABASE_URL}}"

# URLドライバー変換（asyncpg → psycopg）
if [[ "$ALEMBIC_DATABASE_URL" == *"asyncpg"* ]]; then
    export ALEMBIC_DATABASE_URL="${ALEMBIC_DATABASE_URL//asyncpg/psycopg}"
fi

echo "[entrypoint] Running migrations..."
alembic upgrade head || {
    echo "[entrypoint] Migration failed, attempting stamp..."
    alembic stamp head
}

echo "[entrypoint] Starting server..."
exec gunicorn app.main:app \
    --workers="${WEB_CONCURRENCY:-2}" \
    --worker-class=uvicorn.workers.UvicornWorker \
    --bind="0.0.0.0:${PORT:-8000}" \
    --timeout="${GUNICORN_TIMEOUT:-60}"  # 30→60秒に増加
```

---

## 4. 🟡 config.py - Starter Plan最適化

### WHY
- デフォルト設定が開発環境向け
- Supabase + Renderの制限を考慮していない
- 自動データ取得のタイムアウトが短すぎる

### WHAT
- 接続プール設定の最適化
- タイムアウト値の調整（自動取得対応）

### AS-IS
```python
# app/core/config.py
DB_POOL_SIZE: int = 5
DB_MAX_OVERFLOW: int = 5
DB_POOL_RECYCLE: int = 1800
YF_REFETCH_DAYS: int = 7
FETCH_TIMEOUT_SECONDS: int = 8
GUNICORN_TIMEOUT: int = 120
```

### TO-BE
```python
# app/core/config.py
DB_POOL_SIZE: int = 2  # Starter Plan最適値
DB_MAX_OVERFLOW: int = 3
DB_POOL_RECYCLE: int = 900  # 15分（Supabase推奨）
YF_REFETCH_DAYS: int = 7  # 維持

# 自動データ取得対応のタイムアウト設定
FETCH_TIMEOUT_SECONDS: int = 30  # 8→30秒（長期間データ取得対応）
GUNICORN_TIMEOUT: int = 60  # 120→60秒（実用的な値）
REQUEST_TIMEOUT_SECONDS: int = 45  # 15→45秒
YF_REQ_CONCURRENCY: int = 2  # 4→2（API制限対策）
```

---

## 5. 🟡 不要ファイルの削除

### WHY
- メンテナンス性の低下
- どのモジュールを使うべきか不明確
- デプロイサイズの増大

### WHAT
- 未使用・重複モジュールの削除

### AS-IS
```
app/
├── db/
│   ├── queries.py           # ✅ 使用中
│   ├── queries_new.py       # ❌ エラーあり
│   └── queries_optimized.py # ❌ 未使用
├── monitoring/              # ❌ 開発用（本番不要）
└── profiling/               # ❌ 開発用（本番不要）
```

### TO-BE
```
app/
└── db/
    └── queries.py  # メインモジュールのみ維持
```

---

## 6. 🟡 render.yaml環境変数整理

### WHY
- 環境変数が最適化されていない
- デフォルト値が本番向けでない
- 自動データ取得のタイムアウト未設定

### WHAT
- Starter Plan向け最適値設定
- タイムアウト延長

### AS-IS
```yaml
# render.yaml
envVars:
  - key: DB_POOL_SIZE
    value: "3"
  - key: DB_MAX_OVERFLOW
    value: "2"
  - key: GUNICORN_TIMEOUT
    value: "180"
```

### TO-BE
```yaml
# render.yaml
services:
  - type: web
    name: stock-api
    env: docker
    plan: starter  # 明示的に指定
    envVars:
      # Supabase接続（Pooler mode推奨）
      - key: DATABASE_URL
        sync: false
      
      # 接続プール最適化
      - key: DB_POOL_SIZE
        value: "2"
      - key: DB_MAX_OVERFLOW
        value: "3"
      - key: DB_POOL_RECYCLE
        value: "900"
      
      # パフォーマンス設定（自動取得対応）
      - key: GUNICORN_TIMEOUT
        value: "60"
      - key: FETCH_TIMEOUT_SECONDS
        value: "30"
      - key: REQUEST_TIMEOUT_SECONDS
        value: "45"
      - key: WEB_CONCURRENCY
        value: "2"
      - key: YF_REQ_CONCURRENCY
        value: "2"
      
      # API設定（維持）
      - key: API_MAX_SYMBOLS
        value: "50"
      - key: API_MAX_ROWS
        value: "1000000"
```

---

## 7. 🟢 トランザクション管理確認

### WHY
- データ整合性の保証
- エラー時の自動ロールバック

### WHAT
- 既存実装の確認と活用

### AS-IS/TO-BE
```python
# app/api/deps.py - すでに実装済み（維持）
async def get_session() -> AsyncGenerator[AsyncSession, None]:
    SessionLocal = _sessionmaker_for(settings.DATABASE_URL)
    async with SessionLocal() as session:
        try:
            yield session
            if session.in_transaction():
                await session.commit()  # ✅ 自動コミット
        except Exception:
            if session.in_transaction():
                await session.rollback()  # ✅ 自動ロールバック
            raise
```

---

## 8. 🔴 最古日自動検出＆全期間データ取得（新機能）

### WHY
- データが存在しない過去日を指定した場合、現在は空配列を返す
- ユーザーは「データがない」のか「まだ取得していない」のか判断できない
- 手動でデータの存在する期間を探す必要がある

### WHAT
- Yahoo Financeから利用可能な最古日を自動検出
- 要求期間にデータがない場合、利用可能な全期間を自動取得
- 初回は時間がかかるが、確実にデータを提供

### AS-IS
```python
# app/db/queries.py - 現在の動作
async def ensure_coverage(...):
    # 要求された期間のデータ取得を試行
    df = await fetch_prices_df(symbol=symbol, start=start, end=date_to)
    if df is None or df.empty:
        continue  # 静かにスキップ → 空配列を返す
```

### TO-BE
```python
# app/db/queries.py - 新関数追加
async def ensure_coverage_with_auto_fetch(
    session: AsyncSession,
    symbols: Sequence[str],
    date_from: date,
    date_to: date,
    refetch_days: int,
) -> Dict[str, Any]:
    """
    データカバレッジ確保（最古日自動検出付き）
    """
    result_meta = {"fetched_ranges": {}, "row_counts": {}, "adjustments": {}}
    
    for symbol in symbols:
        # DBの現在のカバレッジ確認
        cov = await _get_coverage(session, symbol, date_from, date_to)
        
        if not cov.get("first_date") or cov.get("has_gaps"):
            # Yahoo Financeから最古日を段階的に探索
            actual_start = await find_earliest_available_date(symbol, date_from)
            
            if actual_start > date_from:
                result_meta["adjustments"][symbol] = (
                    f"requested {date_from}, actual {actual_start}"
                )
                logger.warning(
                    f"Symbol {symbol}: Auto-adjusting date_from "
                    f"from {date_from} to {actual_start}"
                )
            
            # 利用可能な全期間のデータを取得
            logger.info(f"Auto-fetching {symbol} from {actual_start} to {date_to}")
            df = await fetch_prices_df(symbol=symbol, start=actual_start, end=date_to)
            
            if df is not None and not df.empty:
                rows = df_to_rows(df, symbol=symbol, source="yfinance")
                if rows:
                    await session.execute(text(upsert_prices_sql()), rows)
                    result_meta["fetched_ranges"][symbol] = {
                        "from": str(actual_start), "to": str(date_to)
                    }
                    result_meta["row_counts"][symbol] = len(rows)
    
    return result_meta

async def find_earliest_available_date(symbol: str, target_date: date) -> date:
    """
    効率的に最古の利用可能日を探索
    """
    import yfinance as yf
    
    # よくある開始日の候補を試行
    test_dates = [
        date(1970, 1, 1),   # Unix epoch
        date(1980, 1, 1),   # 多くの米国株
        date(1990, 1, 1),   # 90年代上場
        date(2000, 1, 1),   # 2000年代
        date(2010, 1, 1),   # 最近のIPO
    ]
    
    for test_date in test_dates:
        if test_date >= target_date:
            try:
                # 30日分の小さなテスト
                df = yf.download(
                    symbol,
                    start=test_date,
                    end=test_date + timedelta(days=30),
                    progress=False,
                    timeout=5
                )
                if not df.empty:
                    # 実際の最古日を取得
                    return df.index[0].date()
            except:
                continue
    
    # デフォルト：要求日付または2000年のどちらか新しい方
    return max(target_date, date(2000, 1, 1))
```

### API側の修正
```python
# app/api/v1/prices.py
@router.get("/prices", response_model=List[PriceRowOut])
async def get_prices(
    symbols: str = Query(...),
    date_from: date = Query(..., alias="from"),
    date_to: date = Query(..., alias="to"),
    auto_fetch: bool = Query(True, description="Auto-fetch all available data if missing"),
    session=Depends(get_session),
):
    # ... validation ...
    
    async with session.begin():
        if settings.ENABLE_AUTO_REGISTRATION:
            await ensure_symbols_registered(session, symbols_list)
        
        t0 = time.perf_counter()
        
        if auto_fetch:
            # 新機能：最古日から全データ自動取得
            fetch_meta = await queries.ensure_coverage_with_auto_fetch(
                session=session,
                symbols=symbols_list,
                date_from=date_from,
                date_to=date_to,
                refetch_days=settings.YF_REFETCH_DAYS,
            )
            
            if fetch_meta.get("adjustments"):
                logger.info(f"Date adjustments applied: {fetch_meta['adjustments']}")
        else:
            # 従来の動作（自動取得なし）
            await queries.ensure_coverage(
                session=session,
                symbols=symbols_list,
                date_from=date_from,
                date_to=date_to,
                refetch_days=settings.YF_REFETCH_DAYS,
            )
        
        # データ取得
        rows = await queries.get_prices_resolved(
            session=session,
            symbols=symbols_list,
            date_from=date_from,
            date_to=date_to,
        )
    
    # パフォーマンス警告
    dt_ms = int((time.perf_counter() - t0) * 1000)
    if dt_ms > 5000:
        logger.warning(f"Slow initial fetch: {dt_ms}ms (expected for new data)")
    
    return rows
```

---

## 📊 修正優先度と実施順序

| 優先度 | ファイル | 作業内容 | 所要時間 |
|--------|----------|----------|----------|
| 1 | queries_new.py | 削除 | 1分 |
| 2 | queries.py | 自動取得機能追加 | 20分 |
| 3 | prices.py | auto_fetchパラメータ追加 | 10分 |
| 4 | engine.py | Supabase接続最適化 | 10分 |
| 5 | entrypoint.sh | 簡潔化 | 15分 |
| 6 | config.py | タイムアウト調整 | 5分 |
| 7 | render.yaml | 環境変数更新 | 5分 |
| 8 | 不要ファイル | 削除 | 5分 |

**合計所要時間: 約70分**

---

## ✅ 期待される効果

### 定量的効果
- **API応答成功率**: 70% → 99%+
- **DB接続エラー**: 10回/日 → 0回/日
- **デプロイ成功率**: 60% → 95%+
- **データ取得成功率**: 50% → 100%（自動調整により）
- **初回データ取得時間**: N/A → 10-30秒（許容範囲）
- **2回目以降の応答時間**: 5秒 → 100ms以下

### 定性的効果
- データが存在しない期間の自動判定により、ユーザー体験向上
- エラーメッセージの明確化によるデバッグ時間短縮
- コードベースの簡潔化によるメンテナンス性向上
- 別アプリからのAPI呼び出し安定性向上

---

## 🚀 実装後の検証項目

### 1. ローカル環境テスト
```bash
docker-compose up --build

# ヘルスチェック
curl http://localhost:8000/healthz

# 通常のデータ取得
curl "http://localhost:8000/v1/prices?symbols=AAPL&from=2024-01-01&to=2024-01-31"

# 自動取得テスト（過去日指定）
curl "http://localhost:8000/v1/prices?symbols=AAPL&from=1990-01-01&to=2024-12-31&auto_fetch=true"
```

### 2. Renderデプロイテスト
```bash
git push origin main
# Render自動デプロイ待機

# 本番環境テスト
curl https://stock-api.onrender.com/healthz

# 自動取得機能テスト（初回は30秒程度かかる）
curl "https://stock-api.onrender.com/v1/prices?symbols=MSFT&from=1980-01-01&to=2024-12-31"
```

### 3. 外部アプリからのテスト
```python
import httpx
import time

# 自動取得機能付きリクエスト
start = time.time()
response = httpx.get(
    "https://stock-api.onrender.com/v1/prices",
    params={
        "symbols": "AAPL,MSFT",
        "from": "1990-01-01",  # 実際のデータより前
        "to": "2024-12-31",
        "auto_fetch": "true"
    },
    timeout=60.0  # タイムアウトを長めに設定
)
elapsed = time.time() - start

assert response.status_code == 200
data = response.json()
assert len(data) > 0
print(f"Fetched {len(data)} records in {elapsed:.2f} seconds")

# 2回目は高速
start = time.time()
response = httpx.get(
    "https://stock-api.onrender.com/v1/prices",
    params={
        "symbols": "AAPL,MSFT",
        "from": "1990-01-01",
        "to": "2024-12-31"
    },
    timeout=10.0
)
elapsed = time.time() - start
assert elapsed < 1.0  # 1秒以内
```

---

## 📝 補足事項

### パフォーマンス期待値
```yaml
# 初回データ取得時間（auto_fetch=true）
短期間（1年）: 3-5秒
中期間（5年）: 5-10秒
長期間（10年）: 10-20秒
全期間（40年+）: 20-40秒

# 2回目以降（DBキャッシュ済み）
全ケース: 50-200ms
```

### 制限事項
- Yahoo Finance APIのレート制限により、大量シンボルの同時取得は避ける
- 初回取得時のタイムアウトに注意（クライアント側も60秒以上に設定推奨）
- Supabase無料プランの場合、DBサイズ制限（500MB）に注意

### ロールバック手順
```bash
# Gitで前のコミットに戻す
git revert HEAD
git push origin main

# または特定のコミットに戻す
git reset --hard <commit-hash>
git push -f origin main
```

---

**作成者**: Stock Data Team  
**最終更新**: 2025年9月6日（自動データ取得機能追加）